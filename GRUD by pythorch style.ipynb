{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "limitation\n",
    "if input_size != hidden_size, GRUD would not works.\n",
    "dropout and bidirectinoal are not working\n",
    "bias True/False is not checked\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # GRUD\n",
    "import math\n",
    "import warnings\n",
    "import itertools\n",
    "import numbers\n",
    "import numpy as np\n",
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUD(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, x_mean=0,\\\n",
    "                 bias=True, batch_first=False, bidirectional=False, dropout_type='mloss', dropout=0):\n",
    "        super(GRUD, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.zeros = torch.autograd.Variable(torch.zeros(input_size))\n",
    "        self.x_mean = torch.autograd.Variable(torch.tensor(x_mean))\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout_type = dropout_type\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        if not isinstance(dropout, numbers.Number) or not 0 <= dropout <= 1 or \\\n",
    "                isinstance(dropout, bool):\n",
    "            raise ValueError(\"dropout should be a number in range [0, 1] \"\n",
    "                             \"representing the probability of an element being \"\n",
    "                             \"zeroed\")\n",
    "        if dropout > 0 and num_layers == 1:\n",
    "            warnings.warn(\"dropout option adds dropout after all but last \"\n",
    "                          \"recurrent layer, so non-zero dropout expects \"\n",
    "                          \"num_layers greater than 1, but got dropout={} and \"\n",
    "                          \"num_layers={}\".format(dropout, num_layers))\n",
    "        \n",
    "        ################################\n",
    "        gate_size = 1 # not used\n",
    "        ################################\n",
    "        \n",
    "        self._all_weights = []\n",
    "        for layer in range(num_layers):\n",
    "            for direction in range(num_directions):\n",
    "                # not used\n",
    "                layer_input_size = input_size if layer == 0 else hidden_size * num_directions\n",
    "                '''\n",
    "                w_ih = Parameter(torch.Tensor(gate_size, layer_input_size))\n",
    "                w_hh = Parameter(torch.Tensor(gate_size, hidden_size))\n",
    "                b_ih = Parameter(torch.Tensor(gate_size))\n",
    "                b_hh = Parameter(torch.Tensor(gate_size))\n",
    "                layer_params = (w_ih, w_hh, b_ih, b_hh)\n",
    "                '''\n",
    "                # decay rates gamma\n",
    "                w_dg_x = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "                w_dg_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "                                \n",
    "                # z\n",
    "                w_xz = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "                w_hz = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "                w_mz = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "                \n",
    "                # r\n",
    "                w_xr = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "                w_hr = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "                w_mr = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "                \n",
    "                # h_tilde\n",
    "                w_xh = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "                w_hh = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "                w_mh = torch.nn.Parameter(torch.Tensor(input_size))\n",
    "                \n",
    "                # y (output)\n",
    "                w_hy = torch.nn.Parameter(torch.Tensor(output_size, hidden_size))\n",
    "                \n",
    "                # bias\n",
    "                b_dg_x = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "                b_dg_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "                b_z = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "                b_r = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "                b_h = torch.nn.Parameter(torch.Tensor(hidden_size))\n",
    "                b_y = torch.nn.Parameter(torch.Tensor(output_size))\n",
    "                \n",
    "                layer_params = (w_dg_x, w_dg_h,\\\n",
    "                                w_xz, w_hz, w_mz,\\\n",
    "                                w_xr, w_hr, w_mr,\\\n",
    "                                w_xh, w_hh, w_mh,\\\n",
    "                                w_hy,\\\n",
    "                                b_dg_x, b_dg_h, b_z, b_r, b_h, b_y)\n",
    "                \n",
    "                suffix = '_reverse' if direction == 1 else ''\n",
    "                param_names = ['weight_dg_x_l{}{}', 'weight_dg_h_l{}{}',\\\n",
    "                               'weight_xz_l{}{}', 'weight_hz_l{}{}','weight_mz_l{}{}',\\\n",
    "                               'weight_xr_l{}{}', 'weight_hr_l{}{}','weight_mr_l{}{}',\\\n",
    "                               'weight_xh_l{}{}', 'weight_hh_l{}{}','weight_mh_l{}{}',\\\n",
    "                               'weight_hy']\n",
    "                if bias:\n",
    "                    param_names += ['bias_dg_x_l{}{}', 'bias_dg_h_l{}{}',\\\n",
    "                                    'bias_z_l{}{}',\\\n",
    "                                    'bias_r_l{}{}',\\\n",
    "                                    'bias_h_l{}{}',\\\n",
    "                                    'bias_y']\n",
    "                param_names = [x.format(layer, suffix) for x in param_names]\n",
    "\n",
    "                for name, param in zip(param_names, layer_params):\n",
    "                    setattr(self, name, param)\n",
    "                self._all_weights.append(param_names)\n",
    "        \n",
    "        self.flatten_parameters()\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def flatten_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets parameter data pointer so that they can use faster code paths.\n",
    "        Right now, this works only if the module is on the GPU and cuDNN is enabled.\n",
    "        Otherwise, it's a no-op.\n",
    "        \"\"\"\n",
    "        any_param = next(self.parameters()).data\n",
    "        if not any_param.is_cuda or not torch.backends.cudnn.is_acceptable(any_param):\n",
    "            return\n",
    "\n",
    "        # If any parameters alias, we fall back to the slower, copying code path. This is\n",
    "        # a sufficient check, because overlapping parameter buffers that don't completely\n",
    "        # alias would break the assumptions of the uniqueness check in\n",
    "        # Module.named_parameters().\n",
    "        all_weights = self._flat_weights\n",
    "        unique_data_ptrs = set(p.data_ptr() for p in all_weights)\n",
    "        if len(unique_data_ptrs) != len(all_weights):\n",
    "            return\n",
    "\n",
    "        with torch.cuda.device_of(any_param):\n",
    "            import torch.backends.cudnn.rnn as rnn\n",
    "\n",
    "            # NB: This is a temporary hack while we still don't have Tensor\n",
    "            # bindings for ATen functions\n",
    "            with torch.no_grad():\n",
    "                # NB: this is an INPLACE function on all_weights, that's why the\n",
    "                # no_grad() is necessary.\n",
    "                torch._cudnn_rnn_flatten_weight(\n",
    "                    all_weights, (4 if self.bias else 2),\n",
    "                    self.input_size, rnn.get_cudnn_mode(self.mode), self.hidden_size, self.num_layers,\n",
    "                    self.batch_first, bool(self.bidirectional))\n",
    "\n",
    "    def _apply(self, fn):\n",
    "        ret = super(GRUD, self)._apply(fn)\n",
    "        self.flatten_parameters()\n",
    "        return ret\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def check_forward_args(self, input, hidden, batch_sizes):\n",
    "        is_input_packed = batch_sizes is not None\n",
    "        expected_input_dim = 2 if is_input_packed else 3\n",
    "        if input.dim() != expected_input_dim:\n",
    "            raise RuntimeError(\n",
    "                'input must have {} dimensions, got {}'.format(\n",
    "                    expected_input_dim, input.dim()))\n",
    "        if self.input_size != input.size(-1):\n",
    "            raise RuntimeError(\n",
    "                'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n",
    "                    self.input_size, input.size(-1)))\n",
    "\n",
    "        if is_input_packed:\n",
    "            mini_batch = int(batch_sizes[0])\n",
    "        else:\n",
    "            mini_batch = input.size(0) if self.batch_first else input.size(1)\n",
    "\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        expected_hidden_size = (self.num_layers * num_directions,\n",
    "                                mini_batch, self.hidden_size)\n",
    "        \n",
    "        def check_hidden_size(hx, expected_hidden_size, msg='Expected hidden size {}, got {}'):\n",
    "            if tuple(hx.size()) != expected_hidden_size:\n",
    "                raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))\n",
    "\n",
    "        if self.mode == 'LSTM':\n",
    "            check_hidden_size(hidden[0], expected_hidden_size,\n",
    "                              'Expected hidden[0] size {}, got {}')\n",
    "            check_hidden_size(hidden[1], expected_hidden_size,\n",
    "                              'Expected hidden[1] size {}, got {}')\n",
    "        else:\n",
    "            check_hidden_size(hidden, expected_hidden_size)\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        s = '{input_size}, {hidden_size}'\n",
    "        if self.num_layers != 1:\n",
    "            s += ', num_layers={num_layers}'\n",
    "        if self.bias is not True:\n",
    "            s += ', bias={bias}'\n",
    "        if self.batch_first is not False:\n",
    "            s += ', batch_first={batch_first}'\n",
    "        if self.dropout != 0:\n",
    "            s += ', dropout={dropout}'\n",
    "        if self.bidirectional is not False:\n",
    "            s += ', bidirectional={bidirectional}'\n",
    "        return s.format(**self.__dict__)\n",
    "    \n",
    "    \n",
    "    def __setstate__(self, d):\n",
    "        super(GRUD, self).__setstate__(d)\n",
    "        if 'all_weights' in d:\n",
    "            self._all_weights = d['all_weights']\n",
    "        if isinstance(self._all_weights[0][0], str):\n",
    "            return\n",
    "        num_layers = self.num_layers\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        self._all_weights = []\n",
    "        for layer in range(num_layers):\n",
    "            for direction in range(num_directions):\n",
    "                suffix = '_reverse' if direction == 1 else ''\n",
    "                weights = ['weight_dg_x_l{}{}', 'weight_dg_h_l{}{}',\\\n",
    "                           'weight_xz_l{}{}', 'weight_hz_l{}{}','weight_mz_l{}{}',\\\n",
    "                           'weight_xr_l{}{}', 'weight_hr_l{}{}','weight_mr_l{}{}',\\\n",
    "                           'weight_xh_l{}{}', 'weight_hh_l{}{}','weight_mh_l{}{}',\\\n",
    "                           'weight_hy',\\\n",
    "                           'bias_dg_x_l{}{}', 'bias_dg_h_l{}{}',\\\n",
    "                           'bias_z_l{}{}', 'bias_r_l{}{}', 'bias_h_l{}{}','bias_y']\n",
    "                weights = [x.format(layer, suffix) for x in weights]\n",
    "                if self.bias:\n",
    "                    self._all_weights += [weights]\n",
    "                else:\n",
    "                    self._all_weights += [weights[:2]]\n",
    "\n",
    "    @property\n",
    "    def _flat_weights(self):\n",
    "        return list(self._parameters.values())\n",
    "\n",
    "    @property\n",
    "    def all_weights(self):\n",
    "        return [[getattr(self, weight) for weight in weights] for weights in self._all_weights]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input.size = (3, 33,49) : num_input or num_hidden, num_layer or step\n",
    "        X = torch.squeeze(input[0]) # .size = (33,49)\n",
    "        Mask = torch.squeeze(input[1]) # .size = (33,49)\n",
    "        Delta = torch.squeeze(input[2]) # .size = (33,49)\n",
    "        Hidden_State = torch.autograd.Variable(torch.zeros(input_size))\n",
    "        \n",
    "        step_size = X.size(1) # 49\n",
    "        #print('step size : ', step_size)\n",
    "        \n",
    "        output = None\n",
    "        h = Hidden_State\n",
    "        for layer in range(num_layers):\n",
    "            \n",
    "            x = torch.squeeze(X[:,layer:layer+1])\n",
    "            m = torch.squeeze(Mask[:,layer:layer+1])\n",
    "            d = torch.squeeze(Delta[:,layer:layer+1])\n",
    "            \n",
    "            # decay rates gamma\n",
    "            w_dg_x = getattr(self, 'weight_dg_x_l' + str(layer))\n",
    "            w_dg_h = getattr(self, 'weight_dg_h_l' + str(layer))\n",
    "                                \n",
    "            #z\n",
    "            w_xz = getattr(self, 'weight_xz_l' + str(layer))\n",
    "            w_hz = getattr(self, 'weight_hz_l' + str(layer))\n",
    "            w_mz = getattr(self, 'weight_mz_l' + str(layer))\n",
    "                \n",
    "            # r\n",
    "            w_xr = getattr(self, 'weight_xr_l' + str(layer))\n",
    "            w_hr = getattr(self, 'weight_hr_l' + str(layer))\n",
    "            w_mr = getattr(self, 'weight_mr_l' + str(layer))\n",
    "                \n",
    "            # h_tilde\n",
    "            w_xh = getattr(self, 'weight_xh_l' + str(layer))\n",
    "            w_hh = getattr(self, 'weight_hh_l' + str(layer))\n",
    "            w_mh = getattr(self, 'weight_mh_l' + str(layer))\n",
    "                \n",
    "            # bias\n",
    "            b_dg_x = getattr(self, 'bias_dg_x_l' + str(layer))\n",
    "            b_dg_h = getattr(self, 'bias_dg_h_l' + str(layer))\n",
    "            b_z = getattr(self, 'bias_z_l' + str(layer))\n",
    "            b_r = getattr(self, 'bias_r_l' + str(layer))\n",
    "            b_h = getattr(self, 'bias_h_l' + str(layer))\n",
    "            \n",
    "            #(4)\n",
    "            gamma_x = torch.exp(-torch.max(self.zeros, (w_dg_x * x + b_dg_x)))\n",
    "            gamma_h = torch.exp(-torch.max(self.zeros, (w_dg_h * h + b_dg_h)))\n",
    "\n",
    "            #(5)\n",
    "            x = m * x + (1 - m) * (gamma_x * x + (1 - gamma_x) * self.x_mean)\n",
    "            \n",
    "            #(6)\n",
    "            if self.dropout == 0:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.nn.functional.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.nn.functional.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.nn.functional.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "                \n",
    "            elif self.dropout_type == 'Moon':\n",
    "                '''\n",
    "                RNNDROP: a novel dropout for rnn in asr(2015)\n",
    "                '''\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.nn.functional.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.nn.functional.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                \n",
    "                h_tilde = torch.nn.functional.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "                \n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "                \n",
    "            elif self.dropout_type == 'Gal':\n",
    "                '''\n",
    "                A Theoretically grounded application of dropout in recurrent neural networks(2015)\n",
    "                '''\n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h = dropout(h)\n",
    "                \n",
    "                print('Gal')\n",
    "                print(h)\n",
    "                \n",
    "                h = gamma_h * h\n",
    "                \n",
    "                z = torch.nn.functional.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.nn.functional.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.nn.functional.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "                \n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "                \n",
    "            elif self.dropout_type == 'mloss':\n",
    "                '''\n",
    "                recurrent dropout without memory loss arXiv 1603.05118\n",
    "                g = h_tilde, p = the probability to not drop a neuron\n",
    "                '''\n",
    "                \n",
    "                h = gamma_h * h\n",
    "                \n",
    "                z = torch.nn.functional.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.nn.functional.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.nn.functional.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "                \n",
    "                dropout = torch.nn.Dropout(p=self.dropout)\n",
    "                h_tilde = dropout(h_tilde)\n",
    "                \n",
    "                h = (1 - z)* h + z*h_tilde\n",
    "                \n",
    "            else:\n",
    "                h = gamma_h * h\n",
    "\n",
    "                z = torch.nn.functional.sigmoid((w_xz*x + w_hz*h + w_mz*m + b_z))\n",
    "                r = torch.nn.functional.sigmoid((w_xr*x + w_hr*h + w_mr*m + b_r))\n",
    "                h_tilde = torch.nn.functional.tanh((w_xh*x + w_hh*(r * h) + w_mh*m + b_h))\n",
    "\n",
    "                h = (1 - z) * h + z * h_tilde\n",
    "            \n",
    "            \n",
    "        w_hy = getattr(self, 'weight_hy')\n",
    "        b_y = getattr(self, 'bias_y')\n",
    "\n",
    "        output = torch.matmul(w_hy, h) + b_y\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
